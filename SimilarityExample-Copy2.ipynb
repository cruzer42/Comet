{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf6d1294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: Machine learning is the study of computer algorithms that improve automatically through experience.Machine learning algorithms build a mathematical model based on sample data, known as training data.The discipline of machine learning employs various approaches to teach computers to accomplish tasks where no fully satisfactory algorithm is available.\n",
      "\n",
      "\n",
      "Similar Documents:\n",
      "\n",
      "\n",
      "Document: Machine learning is closely related to computational statistics, which focuses on making predictions using computers.The study of mathematical optimization delivers methods, theory and application domains to the field of machine learning.\n",
      "Cosine Similarity : 0.22860560787391593\n",
      "\n",
      "\n",
      "Document: Machine learning involves computers discovering how they can perform tasks without being explicitly programmed to do so. It involves computers learning from data provided so that they carry out certain tasks.\n",
      "Cosine Similarity : 0.22581304743529423\n",
      "\n",
      "\n",
      "Document: Machine learning approaches are traditionally divided into three broad categories, depending on the nature of the \"signal\"or \"feedback\" available to the learning system: Supervised, Unsupervised and Reinforcement\n",
      "Cosine Similarity : 0.15314340308039842\n",
      "\n",
      "\n",
      "Document: A software engineer creates programs based on logic for the computer to execute. A software engineer has to be more concernedabout the correctness of the program in all the cases. Meanwhile, a data scientist is comfortable with uncertainty and variability.Developing a machine learning application is more iterative and explorative process than software engineering.\n",
      "Cosine Similarity : 0.12407396777398046\n",
      "\n",
      "\n",
      "Document: Software engineering is the systematic application of engineering approaches to the development of software.Software engineering is a computing discipline.\n",
      "Cosine Similarity : 0.04978528121489196\n",
      "Document: Machine learning is the study of computer algorithms that improve automatically through experience.Machine learning algorithms build a mathematical model based on sample data, known as training data.The discipline of machine learning employs various approaches to teach computers to accomplish tasks where no fully satisfactory algorithm is available.\n",
      "\n",
      "\n",
      "Similar Documents:\n",
      "\n",
      "\n",
      "Document: Machine learning is closely related to computational statistics, which focuses on making predictions using computers.The study of mathematical optimization delivers methods, theory and application domains to the field of machine learning.\n",
      "Euclidean Distance : 1.2420904895586988\n",
      "\n",
      "\n",
      "Document: Machine learning involves computers discovering how they can perform tasks without being explicitly programmed to do so. It involves computers learning from data provided so that they carry out certain tasks.\n",
      "Euclidean Distance : 1.2443367330145854\n",
      "\n",
      "\n",
      "Document: Machine learning approaches are traditionally divided into three broad categories, depending on the nature of the \"signal\"or \"feedback\" available to the learning system: Supervised, Unsupervised and Reinforcement\n",
      "Euclidean Distance : 1.3014273678692956\n",
      "\n",
      "\n",
      "Document: A software engineer creates programs based on logic for the computer to execute. A software engineer has to be more concernedabout the correctness of the program in all the cases. Meanwhile, a data scientist is comfortable with uncertainty and variability.Developing a machine learning application is more iterative and explorative process than software engineering.\n",
      "Euclidean Distance : 1.3235754849845318\n",
      "\n",
      "\n",
      "Document: Software engineering is the systematic application of engineering approaches to the development of software.Software engineering is a computing discipline.\n",
      "Euclidean Distance : 1.378560639787099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ericcruz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "# Sample corpus\n",
    "documents = ['Machine learning is the study of computer algorithms that improve automatically through experience.\\\n",
    "Machine learning algorithms build a mathematical model based on sample data, known as training data.\\\n",
    "The discipline of machine learning employs various approaches to teach computers to accomplish tasks \\\n",
    "where no fully satisfactory algorithm is available.',\n",
    "'Machine learning is closely related to computational statistics, which focuses on making predictions using computers.\\\n",
    "The study of mathematical optimization delivers methods, theory and application domains to the field of machine learning.',\n",
    "'Machine learning involves computers discovering how they can perform tasks without being explicitly programmed to do so. \\\n",
    "It involves computers learning from data provided so that they carry out certain tasks.',\n",
    "'Machine learning approaches are traditionally divided into three broad categories, depending on the nature of the \"signal\"\\\n",
    "or \"feedback\" available to the learning system: Supervised, Unsupervised and Reinforcement',\n",
    "'Software engineering is the systematic application of engineering approaches to the development of software.\\\n",
    "Software engineering is a computing discipline.',\n",
    "'A software engineer creates programs based on logic for the computer to execute. A software engineer has to be more concerned\\\n",
    "about the correctness of the program in all the cases. Meanwhile, a data scientist is comfortable with uncertainty and variability.\\\n",
    "Developing a machine learning application is more iterative and explorative process than software engineering.'\n",
    "]\n",
    "\n",
    "documents_df=pd.DataFrame(documents,columns=['documents'])\n",
    "\n",
    "# removing special characters and stop words from the text\n",
    "stop_words_l=stopwords.words('english')\n",
    "documents_df['documents_cleaned']=documents_df.documents.apply(lambda x: \" \".join(re.sub(r'[^a-zA-Z]',' ',w).lower() for w in x.split() if re.sub(r'[^a-zA-Z]',' ',w).lower() not in stop_words_l) )\n",
    "\n",
    "tfidfvectoriser=TfidfVectorizer()\n",
    "tfidfvectoriser.fit(documents_df.documents_cleaned)\n",
    "tfidf_vectors=tfidfvectoriser.transform(documents_df.documents_cleaned)\n",
    "\n",
    "pairwise_similarities=np.dot(tfidf_vectors,tfidf_vectors.T).toarray()\n",
    "pairwise_differences=euclidean_distances(tfidf_vectors)\n",
    "\n",
    "def most_similar(doc_id,similarity_matrix,matrix):\n",
    "    print (f'Document: {documents_df.iloc[doc_id][\"documents\"]}')\n",
    "    print ('\\n')\n",
    "    print ('Similar Documents:')\n",
    "    if matrix=='Cosine Similarity':\n",
    "        similar_ix=np.argsort(similarity_matrix[doc_id])[::-1]\n",
    "    elif matrix=='Euclidean Distance':\n",
    "        similar_ix=np.argsort(similarity_matrix[doc_id])\n",
    "    for ix in similar_ix:\n",
    "        if ix==doc_id:\n",
    "            continue\n",
    "        print('\\n')\n",
    "        print (f'Document: {documents_df.iloc[ix][\"documents\"]}')\n",
    "        print (f'{matrix} : {similarity_matrix[doc_id][ix]}')\n",
    "\n",
    "most_similar(0,pairwise_similarities,'Cosine Similarity')\n",
    "most_similar(0,pairwise_differences,'Euclidean Distance')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cef43f10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7e4ae15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2  1 10 11 12 20 21 22  2  1 12 23 13 24 14 25  4 26 27  4 15 16  2  1\n",
      " 28 29  7 30  5 31  8 32 33 34 17  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "# tokenize and pad every document to make them of the same size\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts(documents_df.documents_cleaned)\n",
    "tokenized_documents=tokenizer.texts_to_sequences(documents_df.documents_cleaned)\n",
    "tokenized_paded_documents=pad_sequences(tokenized_documents,maxlen=64,padding='post')\n",
    "vocab_size=len(tokenizer.word_index)+1\n",
    "print (tokenized_paded_documents[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "089184c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: tensorflow\n",
      "Version: 2.7.0\n",
      "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
      "Home-page: https://www.tensorflow.org/\n",
      "Author: Google Inc.\n",
      "Author-email: packages@tensorflow.org\n",
      "License: Apache 2.0\n",
      "Location: /opt/anaconda3/lib/python3.7/site-packages\n",
      "Requires: absl-py, astunparse, flatbuffers, gast, google-pasta, grpcio, h5py, keras, keras-preprocessing, libclang, numpy, opt-einsum, protobuf, six, tensorboard, tensorflow-estimator, tensorflow-io-gcs-filesystem, termcolor, typing-extensions, wheel, wrapt\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ea545d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: keras\n",
      "Version: 2.7.0\n",
      "Summary: Deep learning for humans.\n",
      "Home-page: https://keras.io/\n",
      "Author: Keras team\n",
      "Author-email: keras-users@googlegroups.com\n",
      "License: Apache 2.0\n",
      "Location: /opt/anaconda3/lib/python3.7/site-packages\n",
      "Requires: \n",
      "Required-by: keras-utils, tensorflow\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8575363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading pre-trained embeddings, each word is represented as a 300 dimensional vector\n",
    "import gensim\n",
    "W2V_PATH=\"GoogleNews-vectors-negative300.bin.gz\"\n",
    "model_w2v = gensim.models.KeyedVectors.load_word2vec_format(W2V_PATH, binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7089bea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 64, 300)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating embedding matrix, every row is a vector representation from the vocabulary indexed by the tokenizer index. \n",
    "embedding_matrix=np.zeros((vocab_size,300))\n",
    "for word,i in tokenizer.word_index.items():\n",
    "    if word in model_w2v:\n",
    "        embedding_matrix[i]=model_w2v[word]\n",
    "# creating document-word embeddings\n",
    "document_word_embeddings=np.zeros((len(tokenized_paded_documents),64,300))\n",
    "for i in range(len(tokenized_paded_documents)):\n",
    "    for j in range(len(tokenized_paded_documents[0])):\n",
    "        document_word_embeddings[i][j]=embedding_matrix[tokenized_paded_documents[i][j]]\n",
    "document_word_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e113892e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd4fa5a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_index)\n",
    "#tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b35e0818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.08837891,  0.1484375 , -0.06298828, ...,  0.02026367,\n",
       "         0.11621094,  0.17578125],\n",
       "       [ 0.25585938, -0.02209473,  0.02905273, ...,  0.04541016,\n",
       "        -0.33984375, -0.08154297],\n",
       "       ...,\n",
       "       [-0.0612793 , -0.07861328, -0.09326172, ...,  0.05371094,\n",
       "        -0.18554688,  0.22558594],\n",
       "       [-0.07226562, -0.04467773, -0.05224609, ...,  0.09619141,\n",
       "        -0.02478027,  0.33007812],\n",
       "       [ 0.11035156,  0.25585938,  0.03417969, ...,  0.21582031,\n",
       "        -0.13378906, -0.06494141]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#embedding_matrix.shape\n",
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52c5f595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_paded_documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41dbbcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print (document_embeddings.shape)\n",
    "#document_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a2a62d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(92, 300)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#len(embedding_matrix)\n",
    "embedding_matrix.shape\n",
    "#embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68882cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(words)\n",
    "#words\n",
    "#len(document_word_embeddings)\n",
    "#document_word_embeddings\n",
    "#document_word_embeddings=np.zeros((len(tokenized_paded_documents),64,300))\n",
    "#document_embeddings\n",
    "#len(document_embeddings)\n",
    "#document_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "935dcd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(embedding_matrix[tokenizer.word_index[words[0]]])\n",
    "#embedding_matrix[tokenizer.word_index[words[90]]].shape\n",
    "#tokenizer.word_index[words[0]]\n",
    "#len(tokenizer.word_index)\n",
    "#tfidf_vectors[i][j]\n",
    "#tfidf_vectors[0][0]\n",
    "#embedding_matrix[tokenizer.word_index[words[0]]]\n",
    "#test = embedding_matrix[tokenizer.word_index[words[0]]].multiply(tfidf_vectors[0][0])\n",
    "#test = np.multiply(embedding_matrix[tokenizer.word_index[words[0]]],(tfidf_vectors[0][0]))\n",
    "#test = embedding_matrix[tokenizer.word_index[words[0]]]*(tfidf_vectors[0][0])\n",
    "#test = embedding_matrix[tokenizer.word_index[words[j]]]*tfidf_vectors[i][j]\n",
    "#test = embedding_matrix[tokenizer.word_index[words[j]]].dot(tfidf_vectors[i][j])\n",
    "#test\n",
    "#test = tfidf_vectors[i][j].dot(embedding_matrix[tokenizer.word_index[words[j]]])\n",
    "#test\n",
    "#test = np.multiply(embedding_matrix[tokenizer.word_index[words[j]]],tfidf_vectors[i][j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f9eeb2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<6x91 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 117 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#len(document_embeddings)\n",
    "#tfidf_vectors = tfidf_vectors.todense()\n",
    "#tfidf_vectors.get_shape()\n",
    "tfidf_vectors\n",
    "#tfidf_vectors.shape\n",
    "#document_embeddings[i]\n",
    "#embedding_matrix[tokenizer.word_index[words[j]]]\n",
    "#from scipy import sparse\n",
    "#test = sparse.csr_matrix.dot(embedding_matrix[tokenizer.word_index[words[j]]],(tfidf_vectors[i][j]))\n",
    "#test = sparse.csr_matrix(embedding_matrix[tokenizer.word_index[words[j]]].dot(tfidf_vectors[i][j]))\n",
    "#test = embedding_matrix[tokenizer.word_index[words[j]]].dot((tfidf_vectors[i][j]))\n",
    "#test = embedding_matrix[tokenizer.word_index[words[j]]].dot(tfidf_vectors[i][j])\n",
    "#test[1][0]\n",
    "#test\n",
    "#test2\n",
    "#test2 = tfidf_vectors.todense(test[1][0]) \n",
    "#td = tf.sparse_tensor_to_dense(tfidf_vectors)\n",
    "#td = csr_matrix.todense(tfidf_vectors)\n",
    "#td = tfidf_vectors.todense()\n",
    "#td.shape\n",
    "#test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f86a1c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#td = tf.sparse.to_dense(tfidf_vectors)\n",
    "#document_embeddings[0]+= (tfidf_vectors[0][0]).dot(embedding_matrix[tokenizer.word_index[words[0]]])\n",
    "#document_embeddings[0]+= (tfidf_vectors[0][0]).dot(embedding_matrix[tokenizer.word_index[words[0]]])\n",
    "#document_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dfdff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sp.sparse.csr_matrix()*sp.sparse.csr_matrix(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "311a64cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ufunc 'add' output (typecode 'O') could not be coerced to provided output parameter (typecode 'd') according to the casting rule ''same_kind''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kx/m5832p652sd5t8j7p73rn_sr0000gn/T/ipykernel_37754/1608324660.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#        document_embeddings[i]+=embedding_matrix[tokenizer.word_index[words[j]]]*tfidf_vectors[i][j]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mdocument_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+=\u001b[0m \u001b[0membedding_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf_vectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m#        document_embeddings[i]+= tf.sparse_tensor_to_dense(embedding_matrix[tokenizer.word_index[words[j]]]*(tfidf_vectors[i][j])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#        document_embeddings[i] = np.add(document_embeddings[i],embedding_matrix[tokenizer.word_index[words[j]]].dot(tfidf_vectors[i][j]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: ufunc 'add' output (typecode 'O') could not be coerced to provided output parameter (typecode 'd') according to the casting rule ''same_kind''"
     ]
    }
   ],
   "source": [
    "# calculating average of word vectors of a document weighted by tf-idf\n",
    "document_embeddings=np.zeros((len(tokenized_paded_documents),300))\n",
    "words=tfidfvectoriser.get_feature_names()\n",
    "for i in range(len(document_word_embeddings)):\n",
    "    for j in range(len(words)):\n",
    "#        document_embeddings[i]+=embedding_matrix[tokenizer.word_index[words[j]]]*tfidf_vectors[i][j]\n",
    "        document_embeddings[i]+= embedding_matrix[tokenizer.word_index[words[j]]].dot(tfidf_vectors[i][j])\n",
    "#        document_embeddings[i]+= tf.sparse_tensor_to_dense(embedding_matrix[tokenizer.word_index[words[j]]]*(tfidf_vectors[i][j])\n",
    "#        document_embeddings[i] = np.add(document_embeddings[i],embedding_matrix[tokenizer.word_index[words[j]]].dot(tfidf_vectors[i][j]))\n",
    "print (document_embeddings.shape)\n",
    "pairwise_similarities=cosine_similarity(document_embeddings)\n",
    "pairwise_differences=euclidean_distances(document_embeddings)\n",
    "most_similar(0,pairwise_similarities,'Cosine Similarity')\n",
    "most_similar(0,pairwise_differences,'Euclidean Distance')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0a6852",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_similarities=cosine_similarity(document_embeddings)\n",
    "pairwise_differences=euclidean_distances(document_embeddings)\n",
    "most_similar(0,pairwise_similarities,'Cosine Similarity')\n",
    "most_similar(0,pairwise_differences,'Euclidean Distance')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2a5515",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts(documents_df.documents_cleaned)\n",
    "tokenized_documents=tokenizer.texts_to_sequences(documents_df.documents_cleaned)\n",
    "tokenized_paded_documents=pad_sequences(tokenized_documents,maxlen=64,padding='post')\n",
    "vocab_size=len(tokenizer.word_index)+1\n",
    "\n",
    "# reading Glove word embeddings into a dictionary with \"word\" as key and values as word vectors\n",
    "embeddings_index = dict()\n",
    "\n",
    "with open('glove.6B.100d.txt') as file:\n",
    "    for line in file:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    \n",
    "# creating embedding matrix, every row is a vector representation from the vocabulary indexed by the tokenizer index. \n",
    "embedding_matrix=np.zeros((vocab_size,100))\n",
    "\n",
    "for word,i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "# calculating average of word vectors of a document weighted by tf-idf\n",
    "document_embeddings=np.zeros((len(tokenized_paded_documents),100))\n",
    "words=tfidfvectoriser.get_feature_names()\n",
    "\n",
    "# instead of creating document-word embeddings, directly creating document embeddings\n",
    "for i in range(documents_df.shape[0]):\n",
    "    for j in range(len(words)):\n",
    "        document_embeddings[i]+=embedding_matrix[tokenizer.word_index[words[j]]]*tfidf_vectors[i][j]\n",
    "        \n",
    "\n",
    "pairwise_similarities=cosine_similarity(document_embeddings)\n",
    "pairwise_differences=euclidean_distances(document_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b03a20a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
